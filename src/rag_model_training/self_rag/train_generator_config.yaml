# Configuration for Self-RAG Generator training

# Dataset to be used for training
train_file: "earnings_call.json"

# Model to be used for training
model_name_or_path: "meta-llama/Llama-3.1-8B-Instruct"

# Configuration for training
config_name: null
use_lora: false

# Lora specific parameters
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1

# Tokenizer specific parameters
tokenizer_name: null
use_slow_tokenizer: false
max_seq_length: 512

# Training specific parameters
per_device_train_batch_size: 8
learning_rate: 5e-5
weight_decay: 0.0
num_train_epochs: 3
max_train_steps: null
gradient_accumulation_steps: 1
lr_scheduler_type: "linear"
warmup_ratio: 0
seed: 42
preprocessing_num_workers: 4
overwrite_cache: false
checkpointing_steps: null
logging_steps: 500
resume_from_checkpoint: null
with_tracking: false
report_to: "all"
low_cpu_mem_usage: true
use_special_tokens: false
output_dir: "./results" 
