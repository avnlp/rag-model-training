# RQ-RAG: Learning to Refine Queries for Retrieval-Augmented Generation

[Learning to Refine Query for Retrieval Augmented Generation (RQ-RAG)](https://arxiv.org/abs/2404.00610) is a technique to enhance the model by equipping it with capabilities for explicit query rewriting, query decomposition, and query disambiguation.

RQ-RAG draws inspiration from Self-RAG and similarly employs control tokens (special tokens) to guide the model in generating high-quality responses.

We reproduce the code for training the RQ-RAG model based on the implementation provided by the authors at: [https://github.com/chanchimin/RQ-RAG](https://github.com/chanchimin/RQ-RAG).

The code has been rewritten to focus on training new LLMs like Llama-3.2. The usage of command line arguments has been removed and the configuration is now handled through a YAML file for ease of training.

## Project Structure

```bash
rq_rag/
   ├── train_rq_rag.py
   ├── train_rq_rag_config.yaml
   └── requirements.txt
```

## Installation

Install dependencies:

```bash
pip install -r requirements.txt
```

## Training

To train the RQ-RAG model, we first have to expand the vocabulary of the model to include the special tokens.

The following special tokens are added:

- `[S_Rewritten_Query]`: Rewritten query generated by the model based on the retrieved documents and current version of the query.
- `[S_Decomposed_Query]`: Decomposed query generated by the model based on the retrieved documents and current version of the query.
- `[S_Disambiguated_Query]`: Disambiguated query generated by the model based on the retrieved documents and current version of the query.
- `[R_Evidences]`/`[/R_Evidences]`: Used to delimit the retrieved documents which are used as context/evidence for the answer.
- `[S_Response]`: Intermediate response generated by the model based on the retrieved documents and current version of the query.
- `[A_Response]`: Final answer generated by the model based on the retrieved documents and current version of the query.

`[EOS]` Tokens are used to delimit the end of the generation after the use of the special tokens. This prompts the model to generate the next step in the generation process.

### Tree Decoding Strategy

- The model follows a tree decoding strategy through a structured flow.
- This strategy unfolds by controlling expansion paths via special tokens, generating and retrieving queries iteratively:  
   a process of generate → retrieve → generate → retrieve → . . . → answer.
- At each iteration, the model decodes diverse search queries tailored to specific needs—whether to rewrite, decompose, or disambiguate.
- These queries, in turn, fetch distinct contexts, leading to varied expansion paths.

To train the model to explicitly refine queries by rewriting, decomposing, or disambiguating, we train the model using the autoregressive objective on samples with these expansion paths.

The training dataset used is the [RQ-RAG training dataset](https://huggingface.co/datasets/zorowin123/rq_rag) released by the authors. The dataset consists of samples from both single-hop datasets like ARC and multi-hop datasets like Musique and HotpotQA.

To run the training script, use the following command:

```bash
python train_rq_rag.py
```

The training script takes a YAML configuration file as input. The training parameters can be specified in `train_rq_rag_config.yaml`.
